{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read chapters 2 and 3 of statistical learning\n",
    "\n",
    "# History \n",
    "\n",
    "* Started in 1800s with a guy named Francis Galton. Galton was studying the relationship between parents and their children. SPecifically, he investigated the relationship between the heights of fathers and their sons. \n",
    "\n",
    "* Discovered that a man's son tended to be roughly as tall as his father. \n",
    "\n",
    "* However Galton's breakthrough was that the sonn's height tended to be closer to the overall average height of all people.  \n",
    "\n",
    "* For example Shaquille O'Neil. Roughly 7ft1in (2.2Meters). if he has a son, he'll be tall too. However, Shaq is such an anomaly that there is also a good chance that his son will be not as tall as Shaq.  \n",
    "\n",
    "* Turns out this is the case as he does have a son. Shaq's son is pretty tall(6ft7in), but not al tall as Shaq.\n",
    "\n",
    "* Galton called this phoenomenon regression, as in \"a father's son's height tends to regress (or drift towards) the mean (average) height.\"\n",
    "\n",
    "* Simplest possible example: Calculating a regression with only two data points. All We are trying to do when we calculate our regression line is draw a line that's as close to every dot as possible.\n",
    "\n",
    "* for classic linear regression, or \"Least Squares Method\", you only measure the closeness in the \"up and down\" direction. \n",
    "\n",
    "* Now wouldnt it be great if we could apply this same concept to a grpah with more than just two data points? \n",
    "\n",
    "* By doing this, we could take multiple men and their son's heights and do things like tell a man how tall we expect his son to be...before he even has a son! \n",
    "\n",
    "* Our goal with linear regression is to minimize the verticaldistance between all the data points and our line. \n",
    "\n",
    "* so in determining the best line, we are attempting to minimize the distance between all the points and their distance to our line. \n",
    "\n",
    "* lots of different ways to minimize this, (sum of squared errors, sum of absolute errors, etc), but all these methods have a general goal of mimizing this distance. \n",
    "\n",
    "* For example one of the most popular methods is the least squares emthod. Here we have blue data points along an x and y axis.\n",
    "\n",
    "* now we want to fit a linear regression line. How do we decide which line is the best line? \n",
    "\n",
    "* we will use the least squares method which is fitted by minimizing the sum of squares of the residuals. \n",
    "\n",
    "* the residuals for an observation is the difference between the observation (the y-value) and the fitted one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
