{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML \n",
    "\n",
    "* using Introduction to statistical learning by Gareth James as a companion book \n",
    "\n",
    "* free online\n",
    "\n",
    "* Mathmatical theroy from said book will be applied and read\n",
    "\n",
    "* What is ML? \n",
    "\n",
    "* method of data analysis that automates analytical model building. \n",
    "\n",
    "* using algorithms that iteratively learn from data, ML allows computers to find hidden insights without being explicitly programmed where to look.\n",
    "\n",
    "* used for but not limited to: \n",
    "    * Fraud detection, web search results, real time ads on webpages, credit scoring and next best offers, prediction of equipment failures, new pricing models, network intrusion detection, recommendation engines, customer segmentation, text, sentiment analysis, predicting  customer churn, pattern and imgage recognition, email spam filtering, financial modeling and more. \n",
    "    \n",
    "### ML process: \n",
    "\n",
    " * Data aquisition \n",
    " * data cleaning\n",
    " * model training&building/ test data, model testing, repeat as many times as necessary.\n",
    " * model deployment\n",
    " \n",
    "#### Threee types of ML Algorithms:\n",
    "* Supervised learning\n",
    "* unsupervised Learning\n",
    "* Reinforcement Learning\n",
    "\n",
    "\n",
    "##### Supervised Learning\n",
    "* you have labeled data and are trying to group together similar data points based off of features \n",
    "\n",
    "##### Unsupervised Learning\n",
    "* You have unlabeled data and are trying to group together similar points based off of features \n",
    "\n",
    "##### Reinforcement Learning\n",
    "* Algorithm learns to perform an action from experience.\n",
    "\n",
    "\n",
    "------------------------------------------------------------------\n",
    "##### Supervised Learning\n",
    "* supervised Learning algorithms are trained using Labeled examples such as an imput where the desired output is known.\n",
    "\n",
    "* For example a piece of equipment could have data points labeled either 'F' (failed) or 'R' (runs).\n",
    "\n",
    "* The Learning algorithm recieves a set of inputs along with the corresponding correct outputs and the algorithm learns by comparing its actual output with correct outputs to find errors.\n",
    "\n",
    "* it then modifies the model accordingly. \n",
    "\n",
    "\n",
    "* through methods like classification regression prediction and graidient boosting supervisied learning uses patterns to predict the values of the label on addtional unlabeled data.\n",
    "\n",
    "* supervised learning is commonly used in applications where historical predicts likely future events. \n",
    "\n",
    "* example can anticipate when credit card fraud transactions are likely to be fraudulent or when insurance customer is likely to file a claim.\n",
    "\n",
    "* Or it can attempt to predict the price of a house based on different features for houses for which we have historical price data. \n",
    "----------------------------------------------------------------------\n",
    "##### Unsupervised Learning\n",
    "\n",
    "* is used against data that has no historical labels \n",
    "\n",
    "* not told the 'right answer'. algorithm must figure out on it's own \n",
    "\n",
    "* goal is to explore the data and find some structure within. \n",
    "\n",
    "* can find the main attributes that spearate customer segments from each other.\n",
    "\n",
    "* popular techniques include self-organizing maps, nearest-neighbor mapping, k-means clustering and singular value decomposition.\n",
    "\n",
    "* these algorithms are also used to segment text topics, recommend items and identify data outliers.\n",
    "------------------------------------------------------------------------\n",
    "##### Reinforcement Learning\n",
    "* often used for robotics gaming and navigation. \n",
    "* with reinforcement learning , the algorithm discovers through trial and error which actions yiled the greatest rewards. \n",
    "* this type of learning has three primary components the agent (learner or decision maker), enviorment  (everything it interacts with) and actions (what the agent can do)\n",
    "\n",
    "* objective for the agent to choose actions that maximize the expected reward over a given amount of time. \n",
    "\n",
    "* agent will reach the goal much faster by following a good policy.\n",
    "\n",
    "* the goal is to learn the best policy.\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "* for each algorithm or ML topic:\n",
    "    * reading assignment\n",
    "    * light overview of theory\n",
    "    * Domonstration Lecture with Python\n",
    "    * ML project assignment\n",
    "    * overview solution of said project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ML with Python\n",
    "\n",
    "* we will be using SciKit Learn package\n",
    "\n",
    "* Most popular ML package for python and has a lo of algorithms built-in. \n",
    "\n",
    "* install with conda install scikit-learn or pip install scikit-learn\n",
    "\n",
    "* basic structure of how to use scikt learn\n",
    "* but first quick review of ML process (...again):\n",
    "     * Data aquisition \n",
    "     * data cleaning\n",
    "     * model training&building/ test data, model testing, repeat as many times as necessary.\n",
    "     * model deployment\n",
    "     \n",
    "* Going over an example of the process to use SciKit Learn\n",
    "\n",
    "every algorithm in scikit-learn via an \"Estimator\" First you'll import the model, general form is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.family import Model\n",
    "# for example:\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Estimator paramaters: all the params of an estimate can be set when it is instantiated, and have suitable default values.\n",
    " \n",
    " * you can use shift+tab in jupyter to check possible params. \n",
    " \n",
    " example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True)\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(normalize= True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your model created with your parameters, it is time to fit your model on some data! \n",
    "\n",
    "But remember, we should split this data into a training set and a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X,y = np.arange(10).reshape((5,2)),range(5)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [8, 9],\n",
       "       [6, 7]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 5],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have splt the data we can train/fit our model on the training data. \n",
    "\n",
    "this is done through the model.fit() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now the model has been fit and trained on the training data.\n",
    "* The model is ready to predict labels our label or values on the test set! \n",
    "We get the predicted values using the predict method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evaluate our model by comparing our predictions to the correct values.\n",
    "\n",
    "the evaluation method depends on what sort of ML algorithm we are using (Regression, Classification, Clustering, etc.)\n",
    "\n",
    "Let's get a quick recap!\n",
    "\n",
    "Scikit-learn strives to have a uniform interface across all method and we'll see examples of these bellow.\n",
    "\n",
    "given a sciit-learn estimator object named model the following methods are availible. . .\n",
    "\n",
    "* Availible in ALL ESTIMATORS:\n",
    "    * model.fit() : fit training data.\n",
    "    * for supervised learning applications, this accepts two arguments: the data X and the labels y (e.g. mdeol.fit(X,y)).\n",
    "    * For unsupervised learning applications, this accepts only a single argument, the data X (e.g.model.fit(X)).\n",
    "    \n",
    "* Availible in supervised estimators:\n",
    "    * model.predict(): given a trained model, predict the label of a new set of data. this method accepts one arg, the new data X_new (e.g. model.predict(X_new)), and returns the learned label for each object in the array.\n",
    "    * model.predict_proba():For classification problems,some estimators also provde this method, which returns the probability that a new observation has each categorical label. in this case, the label with the highest probability is returned by model.predict(). \n",
    "    * model.score() : for classification or regression problems, most estimators implement a score method. Scores are between 0 and 1, with a larger score indicating a better fit. \n",
    "    \n",
    "* availible in unsupervised estimators\n",
    "    * model.predict() : predict labels in clustering algorithms. \n",
    "    * model.transform(): given an unsupervised model, transform new data into the new basis. This also accepts one argument X_new, and returns new representation of the data based on the unsupervised model. \n",
    "    * model.fit_transform() : some estimators implement this method, which more effectively performs a fit and transform on the same input data. \n",
    "    \n",
    "* How to choose and Algorithm \n",
    "    * Google scikit-learn algorithm cheat sheet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation and simple matrix algebra\n",
    "\n",
    "Chosing notation is always a difficult task we'll adopt the same notational conventions of ESL. \n",
    "\n",
    "N represesent the number of distinct data points or or observations in sample.\n",
    "\n",
    "We will let P denote # of variables that are availible for use in making predictions. \n",
    "\n",
    "Example wage data set consists of 12 variables for 3000 people. N= 3000 observations and p=12 variables \n",
    "\n",
    "p might be quite large.\n",
    "\n",
    "we will let xij represent the value of the jth variabl for the Ith observation where i=1,2...,p. \"i\" will be used to index samples or observations (from 1 to n) and j will be used to index the variables (from 1to p). We let x denote a n*p matrix whose (i,j)th element is Xij. (page 10)  \n",
    "think of x as a spread sheat with n rows p cols\n",
    "\n",
    "rows of x = x1,x2,x3,...xn. \n",
    "\n",
    "i = rows \n",
    "n = # of rows\n",
    "\n",
    "j = cols \n",
    "\n",
    "p = # of cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y hat equals f hat times x. f hat represents the resulting prediction of y. F hat is often treated like a black box in the sense that one is not typically concerned with the exact form of f hat provided that it yields accurate predictions for Y. \n",
    "\n",
    "Accuracy of y hat as a prediction for y depends on two quantities.  reducible error and irreducable error. f hat will not be a perfect estimation for f, this will introduce some error. it is reducable because we can potentially improve f hats accuracy by using the most appropriate statistical learning technique to estimate f. Even if we had a perfect estimate, our predictions would still ahve some error in it. this is because Y is also a function of random error. therefore no matter how well we estimate f we cannot reduce the error by the random error varibale. this is the irreducable error. \n",
    "\n",
    "irreducable error is larger than 0 quantity mantain unmeasured variables that useful in predicting Y since we dont measure them f cannot use them for its prediction the quanityt error may also contain unmeasureable variation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Problem\n",
    "\n",
    "Starting point:\n",
    "    * Outcome measurement Y (also called dependent variable, response, target).\n",
    "    * vectorof p predictor measurement X (also called inputs, regressors, covaiates, features, independent variables).\n",
    "    * In the regression problem, Y is quantitive (e. price, blood pressure)\n",
    "    * in the classification problem, Y takes values in a finite, unordered set ( survived/ died, digit )-9, cancer class of tissue sample).\n",
    "    *  We have training data (x1,y1),....,(xn,yn). These are observations (examples, instances) of these measurements. \n",
    "\n",
    "on the basis of the traning data we would like to:\n",
    "* accurately predict unseen test cases\n",
    "* Understan which inputs affect the outcome, and how.\n",
    "* access the quality of our predictions and inferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Philosophy \n",
    "\n",
    "* it is important to understand the ideas behind the various techniques, in order to know how and when to use them.\n",
    "\n",
    "* one has to understand the simpler methods first in order to grasp the more sophisticated ones. \n",
    "\n",
    "* it is important to accurately assess the performance of a method, to know how well or how badly it is working. (simple often works as well as fancy does.)\n",
    "\n",
    "* this is an exciting reseearch area, having important applications in science, industry and finance. \n",
    "\n",
    "* statistical learning is a fundumental ingredient in the training of a modern data scientist. \n",
    "\n",
    "### Unsupervised learning\n",
    "\n",
    "* No outcome variable, just predictos measured on set of samples.\n",
    "\n",
    "* objective is more fuzzy- find groups of sample that behave similarly, find features that behave similarly, find linear combinations of features with the most variation. \n",
    "\n",
    "* difficult to know how well you are doing. \n",
    "\n",
    "* different from supervised learning, but can be usefula s a pre-processing step for supervised learning. \n",
    "\n",
    "\n",
    "#### The netflix prize\n",
    "\n",
    "* competition started in 2006, oct. training data is rating for 18k movies by 400k customers rating between 1 to 5.\n",
    "\n",
    "* Sparse training data - 98% missing.\n",
    "\n",
    "* objective to predicti the rating for a set of 1 million customer-movie pairs that are missing in training data. \n",
    "\n",
    "* original algorithm achieved a root MSE of 0.953. First team to achieve a 10% imporvement wins one million dollars. \n",
    "\n",
    "### statistical Learning verses Machine Learning\n",
    "\n",
    "* ML arose as a sufield of AI \n",
    "* Statistical Learning arose as a subfield of statistics.\n",
    "* Lots of overlap - focus on supervised and unsupervised problems.\n",
    "    * ML greater emphasis on large scale applications and prediction accuracy. \n",
    "    * statistical learning emphasises models and their interpretability, precision and uncertainty.\n",
    "* the distinction  has become more and more blurred, there is a great deal of cross-fertilization\n",
    "* ML has the upper hand in marketing. (buzzword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is stasistical learning\n",
    "\n",
    "Shown are sales vs tv, radio, and newspaper, with a blue linear-regression line fit seperately to each. Can we predict sales using these three? Perhaps we can do better using a model. \n",
    "\n",
    "$Sales \\thickapprox f(TV, Radio, Newspaper)$\n",
    "\n",
    "\n",
    "## Notation \n",
    "\n",
    "Here sales is a response or target that we wish to predict. We generally refer to the response as Y. TV is a feature, oinput opredictor we name it $X_{1}$ . Radio as $X_{2}$ and so on. we can refer to the input vector collectively as:\n",
    "\n",
    "X=\\begin{pmatrix}\n",
    "X_{1}\\\\ \n",
    "X_{2}\\\\ \n",
    "X_{3}\n",
    "\\end{pmatrix}\n",
    "\n",
    "now we write our model as \n",
    "\n",
    "$Y = f(X)+\\epsilon$\n",
    "\n",
    "where $\\epsilon$ captures measurement errors and other descrepencies. \n",
    "\n",
    "\n",
    "## what is $f(X)$ good for?\n",
    "\n",
    "* with a good$f$ we can make predictions of Y at new points X=x.\n",
    "\n",
    "* we can understand the componenets of $X = (X_{1},X_{2},.....,X_{p})$ are important in explaining $Y$ and which are irrelevant. E.g. Senority and years of education have a big impact on income, but marital Status typically does not. \n",
    "\n",
    "* depending on the complexity of $f$ we may not be able to understand how each component $X_{j}$ of $X$ affects $Y.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there an ideal $f(x)$? In particular, what is a good vlaue for $f(x)$ at any selecte value of $X$, say $X=4$ there can be many $Y$ values at  $X=4$. A good value is \n",
    "\n",
    "$$f(4)=E(Y|X=4)$$\n",
    "\n",
    "$E(Y|X=4)$ means expected value (average) of $Y$ given $X=4$.\n",
    "this ideal $f(x) = E(Y|X=x)$ is called regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The regression function $f(x)$\n",
    "\n",
    "* is also defined for vector $X$; e.g.\n",
    "    $f(x) = f(x_1,x_2,x_3) = E(Y|X_1=x_x, X_2=x_2, X_3=x_3 )$\n",
    "\n",
    "* Is the ideal or optimal predictor of Y with regard to mean-squared prediction error: $f(x) =E(Y|X=x)$ is the function that minimizes $E[(Y-g(X))^2|X=x]$ over all functions $g$ at all points $X = x$. \n",
    "\n",
    "* $\\epsilon = Y - f(x)$ is the Irreducable error. -i.e. even if we knew $f(x)$, we would still make errors in prediction, since at each $X=x$ there is typically a distribution of possible $Y$ values.\n",
    "\n",
    "* for any estimate  $f\\hat(x)$ of $f(x)$, we have:  \n",
    "$$E[(y-f\\hat(X))^2|X=x]=[f(x)-(f\\hat(x)]^2+ Var(\\epsilon)$$\n",
    "\n",
    "    * in the second half of the equation epsilon is Irreducable because by it's given nature it is our error. our function and estimate however are reduceable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to estimate $f$\n",
    "\n",
    "* typically we have few if any data points with $X=4$ exactly.\n",
    "\n",
    "* so we can' compute $E(Y|X=x)$!\n",
    "\n",
    "* relax the definition and let\n",
    "$$F\\hat(x) = Ave(Y|X \\epsilon N(x))$$\n",
    "\n",
    "Where $N(x)$ is some neighborhood of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nearest neighbor averaging can be pretty good for small $p$ -i.e. $p<=4$ and large-ish $N$. \n",
    "* We will discuss smoother versions, such as kernal and spline smoothing later in the course. \n",
    "* Nearest neighbor methods can be lousy when $p$ is large. Reason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions. \n",
    "    * we need to get a reasonable fraction of the $N$ values of $y_i$ to average to bring variance down -e.g. 10%.\n",
    "    * a 10% neighborhood in high dimensions need no longer be local, so we lose the spirit of estimating  $E(Y|X =x)$ by local averaging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric and Structured Models. \n",
    "\n",
    "The Linear model is an important example of a parametric model: \n",
    "$$F_L(X)=\\beta_0 + \\beta_1X_1 + \\beta_2X_2...\\beta_pX_p $$\n",
    "\n",
    "* a linear model is specified in terms of $p+1$ parameters $\\beta_0,\\beta_1,\\beta_2....\\beta_p$. \n",
    "* we estimate the parameters by fitting the model to training data. \n",
    "* although it is almost never correct, a linear model often serves as a good and interpretable approximation to the unknown true function $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see diagrams in video 4\n",
    "\n",
    "A linear model $\\hat{f}_L(X)=\\hat{\\beta}_0+\\hat{\\beta}X$ gives a reasonable fit here.\n",
    "\n",
    "\n",
    "A quadratic model $\\hat{f}_Q(X)=\\hat{\\beta}_0+\\hat{\\beta}_1X+\\hat{\\beta}_2X^2$ fits slightly better. \n",
    "\n",
    "Simulated example. red points are simulated values for income from the model. \n",
    "$$income=f(education,seniority)+\\epsilon$$\n",
    "$f$ is the blue surface.\n",
    "\n",
    "Linear regression model fit to the same area. not nearly as close. \n",
    "\n",
    "$$\\hat{f}_L(education,seniority)=\\hat{\\beta}_0+\\hat{\\beta}_1x * education+ \\hat{\\beta}_2x * seniority$$\n",
    "\n",
    "More fixable regression model(seriously check the video) $\\hat{f}_S(education,seniority) fit to the simulated data. Here we use a technique called a thin-plate-spline to fit a flexible surface. We control the roughness of the fit (chapter 7).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some trade-offs \n",
    "* prediction accuracy vs interpretability. -linear models are easy to iterpret; thin-plate splines are not. \n",
    "\n",
    "* good fit verses over-fit or under-fit. -how do we know when the fit is just right? \n",
    "\n",
    "* Parsimony(simpler) verses black-box. - we often prefer a simpler model involving fewer variables over a black-box predictor involving them all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Model Accuracy. \n",
    "\n",
    "suppose we fit a model $\\hat{f}(x)$ to some training data $Tr=\\{x_i,y_i\\}_{1}^{N}$, and wewish to see how well it performs.\n",
    "* we could compute the average squared prediction error over Tr (training):\n",
    "\n",
    "    $$MSE_{Tr}=Ave_{i\\epsilon Tr}\\left [ y_{i}-\\hat{f}(x_i) \\right ]^2$$\n",
    "    \n",
    "This may be biased toward more overfit models. \n",
    "\n",
    "* instead we should if possible,compute it using fresh TEST data\n",
    "$Te=\\{x_i,y_i\\}_{1}^{M}$\n",
    "\n",
    "$$MSE_{Te}=Ave_{i\\epsilon Te}\\left [ y_{i}-\\hat{f}(x_i) \\right ]^2$$\n",
    "\n",
    "(See video 5) Black curve is truth. red curve on right is $MSE_{Te}$, grey curve is $MSE_{Tr}$. Orange, blue and green curves/squares correspond to fits of different flexibility. \n",
    "\n",
    "here the truth is smoother, so the smoother fit and linear model do really well. \n",
    "\n",
    "here the truth is wiggly and the noise is low so the more flexible fits do the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Varince Trade-off\n",
    "\n",
    "Suppose we have fit a model $\\hat{f}(x)$ to some training data $Tr$, and let $(x_0,y_0)$ be a test observation drawn from the population. If the true model is $Y= f(X) +\\epsilon( with f(x)=E(Y|X=x))$, then\n",
    "\n",
    "$$E(y_0-\\hat{f}(x_0))^2 =Var(\\hat{f}(x_0))+[Bias(\\hat{f}(x_0))]^2+Var(\\epsilon)$$\n",
    "\n",
    "The expectation averages over the viability of y_0 as well as the variability in $Tr$. Note that $Bias(\\hat{f}(x_0))]=E [\\hat{f}(x_0)]-f(x_0).$\n",
    "\n",
    "typically as the flexibility of $\\hat{f}$ increases, it's variance increases, and its bias decreases. So choosing the flexibility based on average test error amounts to a bias variance trade off. See video for further visual examples of Bias-Variance trade off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasification Problems\n",
    "\n",
    "Here the response variable is qualitative -e.g. email is one of $C=(spam,ham)(ham=good email)$, digit class is one of $C = \\{0,1,2....,9\\}$. our goals are to:\n",
    "* Build a classifier $C(X)$ that assigns a class label from $C$ to a future unlabeled observation $X$. \n",
    "\n",
    "* Assess the uncertainty in each classification\n",
    "\n",
    "* Understand the roles of the different predictors amoung $X=(X_1,X_2,...X_p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there an ideal $C(X)$? Suppose the $K$ elements in $C$ are numbered $1,2....,K$. let $p_k(x)=Pr(Y=k|X=x)k=1,2,...,K$.\n",
    "\n",
    "These are conditional class probabilites at $x$; e.g. See little barplot at $x=5$. Then the Bayes optimal classifer at $x$ is \n",
    "$$C(x)=j if p_j(x)=max\\{p_1(x),...,pk(x)\\}$$\n",
    "\n",
    "Nearest neighbor averaging can be used as before. Also breaks down as dimension grows. However the impact on $\\hat{C}(x)$ is less than on $\\hat{p}_k(x), k=1,...,K$\n",
    "\n",
    "## Classification: some details \n",
    "* Typically we measure the performance of $\\hat{C}(x)$ using the msicalulation error rate:\n",
    "\n",
    "$$Err_{Te}=Ave_{i\\epsilon Te}I[y_i\\not\\equiv\\hat{C}(x_i)]$$\n",
    "\n",
    "* The Bayes classifier (using the true $p_k(x)$) has the smallest error (in the population).\n",
    "\n",
    "* Support-vector machines build structured models for $C(x)$.\n",
    "\n",
    "* We will also build structured models for representing the $p_k(x)$. e.g Logistic regression,generalized additive models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning r studio. \n",
    "\n",
    "* built in documentation \n",
    "\n",
    "* pretty graphics\n",
    "\n",
    "* index starts at 1 not 0 like most programming langauges. \n",
    "\n",
    "* ? in front of anything you dont understand and documentation should give you a helping hand for at least the respective function confusing you. \n",
    "\n",
    "x = c(2,7,5) #c means collection\n",
    "\n",
    "y = seq(from=4,length=3, by=3) # starting with 4 make a list of three numbers by 3 ints. \n",
    "\n",
    "?seq #pulls up documentation\n",
    "y # 4,7,10\n",
    "x+y # 6 14 15\n",
    "\n",
    "can add subtract exponents. its a programming language if it can't do math we have a problem.\n",
    "indexing is weird. doesnt start with 0 but 1 if you try zero you get zero\n",
    " x[2:] does not work only x[1:2] does need to finish the colon. \n",
    " \n",
    " x[-2] removes the second index and returns the remaining list. \n",
    " \n",
    " matrix 2 way array. simple way to make a matrix from scratch.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
